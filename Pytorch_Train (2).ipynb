{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter, gaussian_filter\n",
    "import cv2\n",
    " \n",
    "def load_lidar_raster(file_path):\n",
    "    \"\"\"Loads a LiDAR raster file and returns it as a NumPy array.\"\"\"\n",
    "    with rasterio.open(file_path) as src:\n",
    "        data = src.read(1)  # Read the first band\n",
    "        profile = src.profile  # Save metadata for future use\n",
    "    return data, profile\n",
    " \n",
    "def normalize_image(image):\n",
    "    \"\"\"Normalize image to range [0, 255] for better visualization and processing.\"\"\"\n",
    "    #image = (image - np.min(image)) / (np.max(image) - np.min(image)) * 255.0\n",
    "    return image\n",
    " \n",
    "def denoise_lidar(image, method='bilateral'):\n",
    "    \"\"\"Apply noise reduction to LiDAR raster using various filtering methods.\"\"\"\n",
    "    if method == 'median':\n",
    "        return median_filter(image, size=3)\n",
    "    elif method == 'gaussian':\n",
    "        return gaussian_filter(image, sigma=1)\n",
    "    elif method == 'bilateral':\n",
    "        return cv2.bilateralFilter(image, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown denoising method: choose 'median', 'gaussian', or 'bilateral'\")\n",
    " \n",
    "def enhance_edges(image):\n",
    "    \"\"\"Apply edge enhancement using the Sobel filter.\"\"\"\n",
    "    sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    edges = np.sqrt(sobel_x**2 + sobel_y**2)\n",
    "    return normalize_image(edges)\n",
    " \n",
    "def preprocess_lidar(image, method='bilateral'):\n",
    "    \"\"\"Preprocess LiDAR raster by denoising and enhancing edges.\"\"\"\n",
    "    image = normalize_image(image)\n",
    "    denoised = denoise_lidar(image, method)\n",
    "    edges = enhance_edges(denoised)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13115, 10460)\n",
      "(13115, 10460)\n",
      "0\n",
      "128\n",
      "256\n",
      "384\n",
      "512\n",
      "640\n",
      "768\n",
      "896\n",
      "1024\n",
      "1152\n",
      "1280\n",
      "1408\n",
      "1536\n",
      "1664\n",
      "1792\n",
      "1920\n",
      "2048\n",
      "2176\n",
      "2304\n",
      "2432\n",
      "2560\n",
      "2688\n",
      "2816\n",
      "2944\n",
      "3072\n",
      "3200\n",
      "3328\n",
      "3456\n",
      "3584\n",
      "3712\n",
      "3840\n",
      "3968\n",
      "4096\n",
      "4224\n",
      "4352\n",
      "4480\n",
      "4608\n",
      "4736\n",
      "4864\n",
      "4992\n",
      "5120\n",
      "5248\n",
      "5376\n",
      "5504\n",
      "5632\n",
      "5760\n",
      "5888\n",
      "6016\n",
      "6144\n",
      "6272\n",
      "6400\n",
      "6528\n",
      "6656\n",
      "6784\n",
      "6912\n",
      "7040\n",
      "7168\n",
      "7296\n",
      "7424\n",
      "7552\n",
      "7680\n",
      "7808\n",
      "7936\n",
      "8064\n",
      "8192\n",
      "8320\n",
      "8448\n",
      "8576\n",
      "8704\n",
      "8832\n",
      "8960\n",
      "9088\n",
      "9216\n",
      "9344\n",
      "9472\n",
      "9600\n",
      "9728\n",
      "9856\n",
      "9984\n",
      "10112\n",
      "10240\n",
      "10368\n",
      "10496\n",
      "10624\n",
      "10752\n",
      "10880\n",
      "11008\n",
      "11136\n",
      "11264\n",
      "11392\n",
      "11520\n",
      "11648\n",
      "11776\n",
      "11904\n",
      "12032\n",
      "12160\n",
      "12288\n",
      "12416\n",
      "12544\n",
      "12672\n",
      "12800\n",
      "12928\n"
     ]
    }
   ],
   "source": [
    "#!pip install rasterio\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "hillshade_path = \"./South_Clear_Creek/Lidar_DEM_Hillshade/South_Clear_Creek_BareEarth_Hillshade_1m.tif\"\n",
    "roads_path = \"./South_Clear_Creek/Roads_Boundary/South_Clear_Creek_Roads_Mask.tif\"\n",
    "dem_path = \"./South_Clear_Creek/Lidar_DEM_Hillshade/South_Clear_Creek_BareEarth_DEM_1m.tif\"\n",
    "\n",
    "\n",
    "with rasterio.open(hillshade_path) as src:\n",
    "    hillshade = src.read().squeeze().astype(np.float32)\n",
    "    hillshade = (hillshade - hillshade.min()) / (hillshade.max() - hillshade.min())\n",
    "\n",
    "with rasterio.open(roads_path) as src:\n",
    "    roads_mask = src.read().squeeze().astype(np.uint8)\n",
    "    road_mask_binary = (roads_mask > 0).astype(np.uint8)\n",
    "with rasterio.open(dem_path) as src:\n",
    "    array = src.read(masked=True)\n",
    "    metadata = src.profile\n",
    "print(array.shape)\n",
    "print(hillshade.shape)\n",
    "\n",
    "def get_patches(image, mask, dem, patch_size=128, stride=128):\n",
    "    img_patches = []\n",
    "    mask_patches = []\n",
    "    dem_patches = []  # To store patches of the DEM data\n",
    "    h, w = image.shape\n",
    "      # Remove the channel dimension (from shape (1, 13115, 10460) to (13115, 10460))\n",
    "    for i in range(0, h - patch_size + 1, stride):\n",
    "        print(i)\n",
    "        for j in range(0, w - patch_size + 1, stride):\n",
    "            # Extract patches for image, mask, and DEM\n",
    "            img_patch = image[i:i+patch_size, j:j+patch_size]\n",
    "            img_patch = preprocess_lidar(img_patch, method='bilateral')\n",
    "            mask_patch = mask[i:i+patch_size, j:j+patch_size]\n",
    "            # Filter out patches with too few valid mask values\n",
    "            if np.sum(mask_patch) > 10:\n",
    "                img_patches.append(np.expand_dims(img_patch, axis=0))  # Add image patch\n",
    "                mask_patches.append(np.expand_dims(mask_patch, axis=0))  # Add mask patch\n",
    "\n",
    "    return img_patches, mask_patches, dem_patches\n",
    "\n",
    "img_patches, mask_patches, dem_patches = get_patches(hillshade, road_mask_binary, array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, init_features=64):\n",
    "        super(UNet, self).__init__()\n",
    "        features = init_features\n",
    "\n",
    "        self.encoder1 = self.conv_block(in_channels, features)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.encoder2 = self.conv_block(features, features * 2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.encoder3 = self.conv_block(features * 2, features * 4)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.encoder4 = self.conv_block(features * 4, features * 8)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = self.conv_block(features * 8, features * 16)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self.conv_block(features * 16, features * 8)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(features * 8, features * 4)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(features * 4, features * 2)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(features * 2, features)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        return self.final_conv(dec1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "class RoadDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.images[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.masks[idx], dtype=torch.float32)\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_patches = np.array(img_patches)\n",
    "mask_patches = np.array(mask_patches)\n",
    "#dem_patches = np.array(dem_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1990, Accuracy: 0.9853, Test Loss: 0.1008, Test Accuracy: 0.9913\n",
      "Epoch 2/10, Loss: 0.0678, Accuracy: 0.9920, Test Loss: 0.0558, Test Accuracy: 0.9914\n",
      "Epoch 3/10, Loss: 0.0490, Accuracy: 0.9920, Test Loss: 0.0469, Test Accuracy: 0.9914\n",
      "Epoch 4/10, Loss: 0.0431, Accuracy: 0.9920, Test Loss: 0.0435, Test Accuracy: 0.9914\n",
      "Epoch 5/10, Loss: 0.0405, Accuracy: 0.9920, Test Loss: 0.0426, Test Accuracy: 0.9913\n",
      "Epoch 6/10, Loss: 0.0390, Accuracy: 0.9920, Test Loss: 0.0406, Test Accuracy: 0.9914\n",
      "Epoch 7/10, Loss: 0.0382, Accuracy: 0.9920, Test Loss: 0.0400, Test Accuracy: 0.9914\n",
      "Epoch 8/10, Loss: 0.0373, Accuracy: 0.9920, Test Loss: 0.0385, Test Accuracy: 0.9914\n",
      "Epoch 9/10, Loss: 0.0368, Accuracy: 0.9920, Test Loss: 0.0396, Test Accuracy: 0.9914\n",
      "Epoch 10/10, Loss: 0.0361, Accuracy: 0.9920, Test Loss: 0.0383, Test Accuracy: 0.9914\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# Split dataset into 80% train, 20% test\n",
    "train_img, test_img, train_mask, test_mask= train_test_split(\n",
    "    img_patches, mask_patches,  test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = RoadDataset(train_img, train_mask)\n",
    "test_dataset = RoadDataset(test_img, test_mask)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# --- Model setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Compute train accuracy\n",
    "        predicted = (torch.sigmoid(output) > 0.5).float()  # Thresholding\n",
    "        correct_train += (predicted == y).sum().item()\n",
    "        total_train += y.numel()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Compute test accuracy\n",
    "            predicted = (torch.sigmoid(output) > 0.5).float()\n",
    "            correct_test += (predicted == y).sum().item()\n",
    "            total_test += y.numel()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = correct_test / total_test\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "          f\"Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trained_unet_64_stride.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pytorch_patches(image, patch_size=128, stride=128):\n",
    "    patches = []\n",
    "    positions = []\n",
    "    H, W = image.shape\n",
    "    for i in range(0, H - patch_size + 1, stride):\n",
    "        for j in range(0, W - patch_size + 1, stride):\n",
    "            patch = image[i:i+patch_size, j:j+patch_size]\n",
    "            patch = np.expand_dims(patch, axis=(0))  # shape: (1, H, W) for PyTorch\n",
    "            patches.append(patch)\n",
    "            positions.append((i, j))\n",
    "    return patches, positions, (H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_large_image(image, model, device, patch_size=128, stride=128):\n",
    "    model.eval()\n",
    "    patches, positions, (H, W) = get_pytorch_patches(image, patch_size, stride)\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for patch in patches:\n",
    "            x = torch.tensor(patch, dtype=torch.float32).unsqueeze(0).to(device)  # shape: (1, 1, 256, 256)\n",
    "            y = model(x)\n",
    "            pred = torch.sigmoid(y).squeeze().cpu().numpy()  # (256, 256)\n",
    "            predictions.append(pred)\n",
    "\n",
    "    # Reassemble full-size prediction\n",
    "    full_pred = np.zeros((H, W), dtype=np.float32)\n",
    "    count_map = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    for pred, (i, j) in zip(predictions, positions):\n",
    "        full_pred[i:i+patch_size, j:j+patch_size] += pred\n",
    "        count_map[i:i+patch_size, j:j+patch_size] += 1\n",
    "\n",
    "    full_pred /= np.maximum(count_map, 1)\n",
    "    binary_mask = (full_pred > 0.005).astype(np.uint8)\n",
    "    return binary_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\brice\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\brice\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opencv-python) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\brice\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m-\u001b[39m image\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (image\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m image\u001b[38;5;241m.\u001b[39mmin())\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Predict full mask\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m predicted_mask \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_large_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[0;32m     23\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(predicted_mask \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_predicted_mask_with_dem.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 9\u001b[0m, in \u001b[0;36mpredict_large_image\u001b[1;34m(image, model, device, patch_size, stride)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patches:\n\u001b[0;32m      8\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(patch, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# shape: (1, 1, 256, 256)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(y)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# (256, 256)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(pred)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[46], line 48\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m dec3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconv3(dec4)\n\u001b[0;32m     47\u001b[0m dec3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((dec3, enc3), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m dec3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconv2(dec3)\n\u001b[0;32m     51\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((dec2, enc2), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:2822\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2820\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2830\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"trained_unet.pth\", map_location=device))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "hillshade_path = \"./South_Clear_Creek/Lidar_DEM_Hillshade/South_Clear_Creek_BareEarth_Hillshade_1m.tif\"\n",
    "with rasterio.open(hillshade_path) as src:\n",
    "    image = src.read(1)\n",
    "image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "# Predict full mask\n",
    "predicted_mask = predict_large_image(image, model, device)\n",
    "\n",
    "# Save\n",
    "Image.fromarray(predicted_mask * 255).save(\"pytorch_predicted_mask_with_dem.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m-\u001b[39m image\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (image\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m image\u001b[38;5;241m.\u001b[39mmin())\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Predict full mask\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m predicted_mask \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_large_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[0;32m     22\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(predicted_mask \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_predicted_mask_3.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 3\u001b[0m, in \u001b[0;36mpredict_large_image\u001b[1;34m(image, model, device, patch_size, stride)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_large_image\u001b[39m(image, model, device, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 3\u001b[0m     patches, positions, (H, W) \u001b[38;5;241m=\u001b[39m \u001b[43mget_pytorch_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[1;32mIn[50], line 4\u001b[0m, in \u001b[0;36mget_pytorch_patches\u001b[1;34m(image, patch_size, stride)\u001b[0m\n\u001b[0;32m      2\u001b[0m patches \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m positions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m H, W \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, H \u001b[38;5;241m-\u001b[39m patch_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, stride):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, W \u001b[38;5;241m-\u001b[39m patch_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, stride):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "with rasterio.open('Upper_Willow_Creek_BareEarth_Hillshade_1m_1_uint8.tif') as src:\n",
    "    arr = src.read().squeeze().astype(np.uint8)\n",
    "patches, positions, (H, W) = get_pytorch_patches(image, 128, 128)\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "model = UNet(in_channels=1, out_channels=1).to(device)\n",
    " \n",
    "model.load_state_dict(torch.load(\"trained_unet_64_stride.pth\", map_location=device))\n",
    " \n",
    "model.to(device)\n",
    " \n",
    "hillshade_path = \"Upper_Willow_Creek_BareEarth_Hillshade_1m_1.tif\"\n",
    "with rasterio.open(hillshade_path) as src:\n",
    "    image = src.read(1)\n",
    "image = (image - image.min()) / (image.max() - image.min())\n",
    " \n",
    "# Predict full mask\n",
    "predicted_mask = predict_large_image(image, model, device)\n",
    " \n",
    "# Save\n",
    "Image.fromarray(predicted_mask * 255).save(\"pytorch_predicted_mask_3.png\")\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brice\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (100000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\Users\\brice\\AppData\\Local\\Temp\\ipykernel_11732\\1624119094.py:20: RuntimeWarning: invalid value encountered in cast\n",
      "  overlay_uint8 = (overlay * 255).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved overlay image as 'hillshade_with_predicted_roads.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import cv2\n",
    " \n",
    "with rasterio.open(\"Upper_Willow_Creek_BareEarth_Hillshade_1m_1.tif\") as src:\n",
    "    hillshade = src.read(1).astype(np.float32)\n",
    "    hillshade = (hillshade - hillshade.min()) / (hillshade.max() - hillshade.min())\n",
    " \n",
    "prediction = np.array(Image.open(\"pytorch_predicted_mask_3.png\").convert(\"L\"))\n",
    "prediction = (prediction > 127).astype(np.uint8)  # white = road\n",
    " \n",
    "if prediction.shape != hillshade.shape:\n",
    "    prediction = cv2.resize(prediction, (hillshade.shape[1], hillshade.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    " \n",
    "hillshade_rgb = np.stack([hillshade]*3, axis=-1)\n",
    "overlay = hillshade_rgb.copy()\n",
    "overlay[prediction == 1] = [0.0, 0.0, 1.0]  # red overlay where prediction = 1\n",
    " \n",
    "overlay_uint8 = (overlay * 255).astype(np.uint8)\n",
    "Image.fromarray(overlay_uint8).save(\"hillshade_with_predicted_roads.png\")\n",
    " \n",
    "print(\"Saved overlay image as 'hillshade_with_predicted_roads.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
